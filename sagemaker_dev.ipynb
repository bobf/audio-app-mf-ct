{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rk6Sc9PhsWCV"
   },
   "source": [
    "# Audio classification model inference\n",
    "\n",
    "* Model - pretrained fastai2 xresnet18 using fastai2 audio library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N_C3LR2lsWCb"
   },
   "source": [
    "**fastai2_audio**\n",
    "\n",
    "The additional requirements of the fastai2_audio package will be dealt with below, using a clone of the following repo:\n",
    "\n",
    "https://github.com/rbracco/fastai2_audio\n",
    "\n",
    "The demo was run and tested by deploying an SageMaker Notebook instance as per the instructions outlined [here] (https://forums.fast.ai/t/platform-amazon-sagemaker-aws/66020).\n",
    "\n",
    "Note - the above link is only accessible as part of the ongoing fastai course for the time being.\n",
    "\n",
    "\n",
    "## Note re dependencies\n",
    "\n",
    "These are set up using the LifeCycle Configuration for the notebook files. The original fastai2 LifeCycle Configuration provided by Matt McClean https://forums.fast.ai/t/fastai2-sagemaker/66444/6 has been modified to also have the installed of the fastai2 audio github repo:\n",
    "\n",
    "`!pip install git+https://github.com/mikful/fastai2_audio.git`\n",
    "\n",
    "\n",
    "As such, the below Installations of fastai2 and fastai2 audio are not necessary if using this LifeCycle Configuration.\n",
    "\n",
    "However, the installation of the libsndfile is required for to avoid and OSError (this will need to be placed within the LifeCycle Config for the setup at some point.\n",
    "\n",
    "`!conda install -c conda-forge libsndfile --yes`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sqkfnPsxvUG7"
   },
   "source": [
    "## Install fastai2\n",
    "\n",
    "**Note: not required if using fastai2 + fastai2 audio LifeCyCle Config**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 802
    },
    "colab_type": "code",
    "id": "SLc5PWxdvYkb",
    "outputId": "6c34bac0-9fbb-4c2f-c0bd-704e647e99f8"
   },
   "outputs": [],
   "source": [
    "#In SageMaker we need to run this as a  shell commands i.e. with '!' infront of 'pip'\n",
    "#!pip install fastai2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ycpfB-G4sWCd"
   },
   "source": [
    "## Install the fastai2_audio library\n",
    "\n",
    "We need to install the fastai2_audio library to the local kernel/environment for the analysis\n",
    "\n",
    "**Note: not required if using fastai2 + fastai2 audio LifeCyCle Config**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "B7ogGfTcsWCf",
    "outputId": "8b029a0c-1065-4c04-bc43-4877420e3eea",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#In Colab we need to run this as a shell command i.e. with '!' infront of 'pip'\n",
    "\n",
    "#!pip install git+https://github.com/mikful/fastai2_audio.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F2gUqCfWsWCm"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /home/ec2-user/SageMaker/.env/fastai2\n",
      "\n",
      "  added / updated specs:\n",
      "    - libsndfile\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    gettext-0.19.8.1           |       h5e8e0c9_1         3.5 MB  conda-forge\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:         3.5 MB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  gettext            conda-forge/linux-64::gettext-0.19.8.1-h5e8e0c9_1\n",
      "  libflac            conda-forge/linux-64::libflac-1.3.3-he1b5a44_0\n",
      "  libogg             conda-forge/linux-64::libogg-1.3.2-h516909a_1002\n",
      "  libsndfile         conda-forge/linux-64::libsndfile-1.0.28-he1b5a44_1000\n",
      "  libvorbis          conda-forge/linux-64::libvorbis-1.3.6-he1b5a44_2\n",
      "  python_abi         conda-forge/linux-64::python_abi-3.6-1_cp36m\n",
      "\n",
      "The following packages will be UPDATED:\n",
      "\n",
      "  ca-certificates     pkgs/main::ca-certificates-2020.1.1-0 --> conda-forge::ca-certificates-2020.4.5.1-hecc5488_0\n",
      "\n",
      "The following packages will be SUPERSEDED by a higher-priority channel:\n",
      "\n",
      "  certifi              pkgs/main::certifi-2020.4.5.1-py36_0 --> conda-forge::certifi-2020.4.5.1-py36h9f0ad1d_0\n",
      "  openssl              pkgs/main::openssl-1.1.1g-h7b6447c_0 --> conda-forge::openssl-1.1.1g-h516909a_0\n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "gettext-0.19.8.1     | 3.5 MB    | ##################################### | 100% \n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: done\n"
     ]
    }
   ],
   "source": [
    "# Solving an OSError problem with Librosa SoundFile dependency (libsndfile)\n",
    "# SageMaker/GCP Only\n",
    "\n",
    "!conda install -c conda-forge libsndfile --yes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hWCdC9SesWCs"
   },
   "source": [
    "## Load Pretrained Model (from Colab) and Perform Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CjNZwPO-sWCu"
   },
   "outputs": [],
   "source": [
    "from fastai2.vision.all import *\n",
    "from fastai2_audio.core import *\n",
    "from fastai2_audio.augment import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_x(r): return r['fname']\n",
    "def get_y(r): return r['labels'].split(',') # split labels on ','\n",
    "dblock = DataBlock(get_x = get_x, get_y = get_y)\n",
    "dsets = dblock.datasets(df_combined)\n",
    "dsets.train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DBMelSpec = SpectrogramTransformer(mel=True, to_db=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = AudioConfig.BasicMelSpectrogram()\n",
    "aud2spec = AudioToSpec.from_cfg(cfg)\n",
    "aud2spec.settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's change the settings to see the impact\n",
    "aud2spec = DBMelSpec(sample_rate= 16000, f_max=None, f_min=20, n_mels=128, n_fft=1024, hop_length=128, top_db=90)\n",
    "aud2spec.settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_tfms = [RemoveSilence(), CropSignal(3000, pad_mode='Repeat'), aud2spec, MaskTime(num_masks=1, size=100), MaskFreq(num_masks=1, size=10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# needs redefining fors test only\n",
    "\n",
    "dblock = DataBlock(blocks=(AudioBlock, MultiCategoryBlock),\n",
    "                    splitter=RandomSplitter(),\n",
    "                    get_x=get_x,\n",
    "                    get_y=get_y,\n",
    "                    item_tfms = item_tfms)\n",
    "\n",
    "# dsets = dblock.datasets(df_curated)\n",
    "dsets = dblock.datasets(df_combined)\n",
    "dsets.train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = dblock.dataloaders(df_combined, bs=32) # bs= batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load pretrained 1-channel xresnet18 with multi-accuracy\n",
    "\n",
    "# Custom cnn model created from pretrained xresnet18 (smaller model for inference speed)\n",
    "# 1 input channel and 80 output nodes\n",
    "# torch.nn.BCEWithLogitsLoss() = Binary Cross Entropy Loss from pytorch\n",
    "# accuracy_multi for multi label\n",
    "\n",
    "model = create_cnn_model(xresnet18, n_in=1, n_out=80, pretrained=True)\n",
    "\n",
    "learn = Learner(dls, model, BCEWithLogitsLossFlat(), metrics=accuracy_multi) # pass custom model to Learner\n",
    "\n",
    "learn.load('xresnet50-stage-2-model-finetuned.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define test file path in S3\n",
    "df_fname = '../data/test/' + df_fnames.fname\n",
    "print(df_fname)\n",
    "\n",
    "#create new dataloaders\n",
    "dl = learn.dls.test_dl(df_fnames)\n",
    "    \n",
    "# predict using tta    \n",
    "preds, targs = learn.tta(dl=dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xz0d72WcsWGp"
   },
   "source": [
    "## Export the model and upload to S3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1SP0fhq6sWGq"
   },
   "source": [
    "Now that we have trained our model we will export it using the learner method `export()` and upload the exported model to S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "g7YXuARWsWGs"
   },
   "outputs": [],
   "source": [
    "learn.export()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "18UaCxaKsWGw"
   },
   "source": [
    "Now let's create a tarfile for our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8K0jE1znsWGz"
   },
   "outputs": [],
   "source": [
    "import tarfile\n",
    "with tarfile.open(path/'model.tar.gz', 'w:gz') as f:\n",
    "    f.add(path/'export.pkl', arcname='model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cIoEqpzxsWG4"
   },
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "sess = sagemaker.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GoPK3YYysWG8"
   },
   "outputs": [],
   "source": [
    "prefix = 'audio-app-mf-ct'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "leG1Y1Q9sWHA"
   },
   "source": [
    "Now we will upload the model to the default S3 bucket for sagemaker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CpwBMVogsWHB"
   },
   "outputs": [],
   "source": [
    "model_location = sess.upload_data(str(path/'model.tar.gz'), key_prefix=prefix)\n",
    "model_location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ae8WqB6OsWHF"
   },
   "source": [
    "## Script for model inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F6sQdVtrsWHG"
   },
   "source": [
    "SageMaker invokes the main function defined within your training script for training. When deploying your trained model to an endpoint, the `model_fn()` is called to determine how to load your trained model. The `model_fn()` along with a few other functions list below are called to enable predictions on SageMaker.\n",
    "\n",
    "### [Predicting Functions](https://github.com/aws/sagemaker-pytorch-containers/blob/master/src/sagemaker_pytorch_container/serving.py)\n",
    "* `model_fn(model_dir)` - loads your model.\n",
    "* `input_fn(serialized_input_data, content_type)` - deserializes predictions to predict_fn.\n",
    "* `output_fn(prediction_output, accept)` - serializes predictions from predict_fn.\n",
    "* `predict_fn(input_data, model)` - calls a model on data deserialized in input_fn.\n",
    "\n",
    "Here is the full code in a file `serve.py` showing implementations of the 4 key functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1yGJoE9bsWHH"
   },
   "outputs": [],
   "source": [
    "!pygmentize scripts/serve.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4_p0MGBxsWHL"
   },
   "source": [
    "## Deploy locally to test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jYT5WychsWHM"
   },
   "source": [
    "Before deploying to Amazon SageMaker we want to verify that the endpoint is working properly. The Amazon SageMaker Python SDK allows us to deploy locally to the Notebook instance using Docker. We will create the model then specify the parameter `instance_type` to be `local` telling the SDK to deploy locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "blQEchj_sWHN"
   },
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorchModel\n",
    "\n",
    "model = PyTorchModel(model_data=model_location,\n",
    "                     role=role,\n",
    "                     framework_version='1.4.0',\n",
    "                     entry_point='serve.py', \n",
    "                     source_dir='scripts')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bWgN9-YUsWHQ"
   },
   "source": [
    "Now that we have created the model we will deploy locally to test. It may take a while to run the first time as we need to download a Docker image to our notebook instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7TKQ_WACsWHR"
   },
   "outputs": [],
   "source": [
    "predictor = model.deploy(initial_instance_count=1, instance_type='local')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6uOjLVQksWHZ"
   },
   "source": [
    "Now we can test out our endpoint. We will download a cat images from the internet and save locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jj9OEL2bsWHa"
   },
   "outputs": [],
   "source": [
    "! [ -d tmp ] || mkdir tmp\n",
    "! wget -q -O tmp/british-shorthair.jpg https://cdn1-www.cattime.com/assets/uploads/2011/12/file_2744_british-shorthair-460x290-460x290.jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_jDltlVYsWHe"
   },
   "outputs": [],
   "source": [
    "img = Image.open('tmp/british-shorthair.jpg')\n",
    "img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YmCko7bfsWHh"
   },
   "source": [
    "Now we can call our local endpoint to ensure it is working and provides us the correct result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CqWJBsF4sWHi"
   },
   "outputs": [],
   "source": [
    "from sagemaker.predictor import json_serializer, json_deserializer\n",
    "\n",
    "predictor.accept = 'application/json'\n",
    "predictor.content_type = 'application/json'\n",
    "\n",
    "predictor.serializer = json_serializer\n",
    "predictor.deserializer = json_deserializer\n",
    "\n",
    "response = predictor.predict( { \"url\": \"https://cdn1-www.cattime.com/assets/uploads/2011/12/file_2744_british-shorthair-460x290-460x290.jpg\" })\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HcaEoVm4sWHm"
   },
   "source": [
    "Once you are happy that the endpoint is working suceessully you can shut it down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1LzkT9NDsWHn"
   },
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VlbLIQoQsWHr"
   },
   "source": [
    "## Deploy to SageMaker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Qlyi0ZmzsWHt"
   },
   "source": [
    "Once we have verified that the script is working successfully on our locally deployed endpoint we can deploy our model to Amazon SageMaker so that it can be used in a production application. The code is almost exactly the same as deploying locally except that when we call `model.deploy()` we will change the instance type to an Amazon SageMaker valid instance type (e.g. `ml.m5.xlarge`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hfZqQGEQsWHw"
   },
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorchModel\n",
    "\n",
    "model = PyTorchModel(model_data=model_location,\n",
    "                     role=role,\n",
    "                     framework_version='1.4.0',\n",
    "                     entry_point='serve.py', \n",
    "                     source_dir='scripts')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LiNTlWaksWHz"
   },
   "source": [
    "Now let's deploy our SageMaker endpoint. It will take a few min to provision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UrEsA37asWH2"
   },
   "outputs": [],
   "source": [
    "predictor = model.deploy(initial_instance_count=1, instance_type='ml.m5.xlarge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3HAXGIxUsWH6"
   },
   "outputs": [],
   "source": [
    "img = Image.open('tmp/british-shorthair.jpg')\n",
    "img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l1FCkQ1isWH9"
   },
   "source": [
    "Now let's test our remote endpoint running on SageMaker hosting services."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OqTrUNPnsWH9"
   },
   "outputs": [],
   "source": [
    "from sagemaker.predictor import json_serializer, json_deserializer\n",
    "\n",
    "predictor.accept = 'application/json'\n",
    "predictor.content_type = 'application/json'\n",
    "\n",
    "predictor.serializer = json_serializer\n",
    "predictor.deserializer = json_deserializer\n",
    "\n",
    "response = predictor.predict( { \"url\": \"https://cdn1-www.cattime.com/assets/uploads/2011/12/file_2744_british-shorthair-460x290-460x290.jpg\" })\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aFrXfbOlsWIB"
   },
   "source": [
    "## Optional: delete endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Lkl5ySrQsWIC"
   },
   "source": [
    "If you do not want to keep the endpoint up and running then remember to delete it to avoid incurring further costs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hH-iPvGssWID"
   },
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "khBWm2CXsWIG"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B7Q8QPciaNhQ"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "g5ZLivT8Ie_M",
    "F6sQdVtrsWHG",
    "4_p0MGBxsWHL",
    "VlbLIQoQsWHr",
    "aFrXfbOlsWIB"
   ],
   "include_colab_link": true,
   "name": "model_colab dev.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "conda_fastai2",
   "language": "python",
   "name": "conda_fastai2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
